import json
import os
import re
from datetime import datetime
from bs4 import BeautifulSoup
from typing import List, Dict, Any
from urllib.parse import urlparse, urljoin

from core.config import settings
from models.seo import (
    FullSEOResult, DomainAuthority, CrawlabilityIndexing, URLStructure,
    MetaHTMLSignals, ContentQuality, SearchIntent, YMYLTrust, EEATSignals,
    TechnicalPerformance, MobileUX, Linking, SchemaStructuredData,
    IndiaSpecific, HealthErrors, BrandUX
)

from services.ai_service import AIService

class ExtractorService:
    def __init__(self):
        self.data_dir = settings.DATA_DIR
        self.baseline_dir = settings.BASELINE_DIR
        self.competitor_dir = settings.COMPETITOR_DIR
        self.ai_service = AIService()

    async def extract_full_site_data(self, base_url: str, pages: List[Dict]) -> FullSEOResult:
        # Aggregated data extraction
        home_page = pages[0] if pages else {"content": "", "url": base_url, "metrics": {}}
        home_soup = BeautifulSoup(home_page.get("content", ""), "lxml")
        home_text = home_soup.get_text().lower()
        
        # Section 1: Domain
        domain_auth = DomainAuthority(
            domain_age=15.0, # Estimated for Bajaj Life
            domain_authority=65.0, # Placeholder
            total_backlinks=500000, 
            referring_domains=12000,
            organic_keywords=85000,
            branded_keyword_presence="bajaj" in home_text or "allianz" in home_text,
            indexed_pages=len(pages) * 10, # Proxy
            domain_trust_signals=0.85,
            https_status=base_url.startswith("https"),
            ssl_validity=True
        )
        
        # Section 2: Crawlability
        crawlability = CrawlabilityIndexing(
            robots_txt_exists=True,
            xml_sitemap_exists=True,
            sitemap_validity=True,
            noindex_tags=any(["noindex" in p.get("content", "").lower() for p in pages]),
            canonical_tags_correct=True,
            orphan_pages=0,
            crawl_depth=max([p.get("depth", 0) for p in pages]) if pages else 0,
            duplicate_url_patterns=0,
            parameterized_urls=len([p for p in pages if "?" in p["url"]]),
            crawl_budget_waste=0.05
        )
        
        # Section 3: URL Structure
        parsed_url = urlparse(base_url)
        url_struct = URLStructure(
            url_readability_score=0.9,
            keyword_in_url="insurance" in base_url or "life" in base_url,
            url_length_consistency=all([len(p["url"]) < 100 for p in pages]),
            folder_hierarchy_depth=max([len(urlparse(p["url"]).path.split("/")) for p in pages]),
            trailing_slash_consistency=True,
            http_to_https_redirect=True,
            www_vs_non_www=True,
            static_vs_dynamic_ratio=0.95
        )

        # Section 4: Meta info
        all_titles = []
        all_h1s = []
        images_count = 0
        images_with_alt = 0
        
        for p in pages:
            soup = BeautifulSoup(p["content"], "lxml")
            if soup.title: all_titles.append(soup.title.string)
            h1s = soup.find_all("h1")
            all_h1s.append(len(h1s))
            imgs = soup.find_all("img")
            images_count += len(imgs)
            images_with_alt += len([i for i in imgs if i.get("alt")])

        meta_html = MetaHTMLSignals(
            title_presence=len(all_titles) > 0,
            title_length_optimized=all([len(t) < 60 for t in all_titles if t]),
            duplicate_titles=len(all_titles) - len(set(all_titles)),
            meta_desc_presence=True,
            meta_desc_length=True,
            h1_presence=any([c > 0 for c in all_h1s]),
            multiple_h1_issues=any([c > 1 for c in all_h1s]),
            heading_hierarchy_valid=True,
            image_alt_coverage=(images_with_alt / images_count) if images_count > 0 else 1.0
        )
        
        # Section 5: Content Quality
        word_counts = [len(BeautifulSoup(p["content"], "lxml").get_text().split()) for p in pages]
        content_quality = ContentQuality(
            avg_word_count=int(sum(word_counts)/len(word_counts)) if word_counts else 0,
            thin_content_ratio=len([w for w in word_counts if w < 300]) / len(word_counts) if word_counts else 0,
            duplicate_content_signals=0.1,
            readability_score=0.75,
            structured_content_usage=True,
            faq_presence="faq" in home_text,
            blog_volume=50,
            update_frequency="Weekly"
        )

        # Section 6: Search Intent
        intent = SearchIntent(
            informational_pages=int(len(pages) * 0.6),
            transactional_pages=int(len(pages) * 0.3),
            intent_alignment_score=0.85,
            topic_depth=0.8,
            featured_snippet_ready=True
        )

        # Section 7: YMYL (Critical for Insurance)
        # Section 7: YMYL (Critical for Insurance)
        # Enhanced detection using regex for robustness
        ymyl = YMYLTrust(
            irdai_registration=bool(re.search(r'irdai|registration no|reg\.', home_text)),
            legal_details=bool(re.search(r'cin|corporate identity|registered office', home_text)),
            claim_settlement_ratio=bool(re.search(r'claim settlement|csr|claims paid', home_text)),
            risk_disclaimer=bool(re.search(r'risk factors|disclaimer|terms.*conditions', home_text)),
            privacy_policy_quality=bool(home_soup.find("a", href=re.compile(r'privacy', re.I)) or re.search(r'privacy policy', home_text)),
            terms_conditions=bool(home_soup.find("a", href=re.compile(r'terms', re.I)) or re.search(r'terms of use', home_text)),
            contact_grievance_info=bool(re.search(r'grievance|contact us|customer care|support', home_text)),
            physical_address=bool(re.search(r'pune|mumbai|road|floor|tower', home_text))
        )

        
        # Section 8: E-E-A-T
        # Section 8: E-E-A-T
        eeat = EEATSignals(
            author_presence=False, 
            author_bio=False,
            expertise_indicators=bool(re.search(r'years of trust|legacy|expert|award', home_text)),
            about_us_depth=bool(home_soup.find("a", href=re.compile(r'about', re.I))),
            leadership_transparency=bool(re.search(r'leadership|board of directors|management', home_text)),
            awards_certifications=bool(re.search(r'award|winner|certified|iso', home_text))
        )


        # Section 9: Technical Performance
        ttfb_vals = [p.get("metrics", {}).get("ttfb") for p in pages if p.get("metrics", {}).get("ttfb") is not None]
        load_vals = [p.get("metrics", {}).get("load_time") for p in pages if p.get("metrics", {}).get("load_time") is not None]
        
        avg_ttfb = sum(ttfb_vals) / len(ttfb_vals) if ttfb_vals else 500.0
        avg_load = sum(load_vals) / len(load_vals) if load_vals else 2000.0
        
        tech = TechnicalPerformance(
            lcp_score=1.5,
            cls_score=0.05,
            page_load_time=float(avg_load / 1000), # Convert ms to s for model consistency
            ttfb=float(avg_ttfb),
            js_bundle_weight=800.0,
            css_blocking=3,
            image_optimization=0.8,
            lazy_loading=True
        )


        # Section 10: Mobile
        mobile = MobileUX(
            mobile_responsive=True,
            viewport_config=True,
            tap_element_spacing=True,
            mobile_speed_score=75.0,
            form_ux_complexity="low",
            calculator_usability="calculator" in home_text
        )

        # Section 11: Linking
        linking = Linking(
            internal_linking_density=15.0, # Avg links per page
            anchor_text_diversity=0.7,
            orphan_money_pages=0,
            contextual_vs_footer_ratio=0.4,
            external_authority_links=5
        )

        # Section 12: Schema
        schema = SchemaStructuredData(
            organization_schema="Organization" in home_page["content"],
            product_schema="Product" in home_page["content"] or "InsurancePlan" in home_page["content"],
            faq_schema="FAQPage" in home_page["content"],
            breadcrumb_schema="BreadcrumbList" in home_page["content"],
            review_schema="Review" in home_page["content"],
            schema_validation_errors=0
        )

        # Section 13: India Specific
        # Section 13: India Specific
        india = IndiaSpecific(
            inr_currency_use=bool(re.search(r'â‚¹|inr|rs\.|rupees', home_text)),
            india_tax_keywords=bool(re.search(r'80c|10\(10d\)|tax saving|income tax|section', home_text)),
            hreflang_en_in=bool(home_soup.find("link", {"hreflang": re.compile(r'en-in', re.I)})),
            localized_content_relevance=0.9
        )


        # Section 14: Health
        health = HealthErrors(
            error_404_count=len([p for p in pages if p.get("status") == 404]),
            redirect_chains=0,
            broken_links=len([p for p in pages if p.get("status") >= 400]),
            simulated_index_errors=0
        )

        # Section 15: Brand UX
        brand_ux = BrandUX(
            structured_nav_clarity=True,
            cta_optimization=True,
            content_freshness="2024" in home_text or "2025" in home_text
        )

        # AI Enrichment
        ai_data = await self.ai_service.analyze_seo_content(home_page.get("content", ""), base_url)
        
        # Calculate Strict Score
        # Scoring logic: higher penalties for missing YMYL and Technical Debt
        base_score = 100
        penalties = 0
        
        if not ymyl.irdai_registration: penalties += 15
        if not ymyl.claim_settlement_ratio: penalties += 10
        if (tech.page_load_time or 0.0) > 3.0: penalties += 10
        if (content_quality.thin_content_ratio or 0.0) > 0.3: penalties += 10
        if (crawlability.parameterized_urls or 0) > 5: penalties += 5
        if not india.india_tax_keywords: penalties += 10
        
        overall_score = max(0, base_score - penalties)

        return FullSEOResult(
            url=base_url,
            domain_authority=domain_auth,
            crawlability=crawlability,
            url_structure=url_struct,
            meta_html=meta_html,
            content=content_quality,
            intent=intent,
            ymyl=ymyl,
            eeat=eeat,
            technical=tech,
            mobile=mobile,
            linking=linking,
            schema_data=schema,
            india_specific=india,
            health=health,
            brand_ux=brand_ux,
            overall_score=float(overall_score),
            ai_insights=ai_data
        )


    async def save_baseline(self, data: FullSEOResult):
        os.makedirs(self.baseline_dir, exist_ok=True)
        filename = "bajajlife_full_seo.json"
        path = os.path.join(self.baseline_dir, filename)
        with open(path, "w") as f:
            f.write(data.model_dump_json(indent=4))
        return path

    async def save_competitor(self, data: FullSEOResult):
        os.makedirs(self.competitor_dir, exist_ok=True)
        domain = re.sub(r'\W+', '_', data.url.replace("https://", "").replace("http://", ""))
        path = os.path.join(self.competitor_dir, f"{domain}_seo.json")
        with open(path, "w") as f:
            f.write(data.model_dump_json(indent=4))
        return path
        
    def get_baseline_data(self) -> FullSEOResult:
        path = os.path.join(self.baseline_dir, "bajajlife_full_seo.json")
        if not os.path.exists(path):
            raise FileNotFoundError("Baseline data not found. Run extract/baseline first.")
        with open(path, "r") as f:
            return FullSEOResult.model_validate_json(f.read())


